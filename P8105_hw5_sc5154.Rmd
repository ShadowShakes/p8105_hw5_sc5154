---
title: "P8105_hw5_sc5154"
author: "Shaohan Chen"
date: "2022-11-16"
output: github_document
---

This is the solution of P8105 Data Science Homework5.

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```

```{r}
library(tidyverse)
```

## Problem 1

The goal of this problem is:

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.  
Then we make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

Let's start with loading the dataset.

```{r}
df_par = 
  tibble(
    file_name = list.files("./data_p1/"),
    file_path = str_c("./data_p1/", file_name)
   ) %>%
  mutate(data = map(file_path, read_csv)) %>%
  unnest()
```

Then let's tidy the dataset.

```{r}
df_par = 
  df_par %>%
  mutate(
    file_name = str_remove(file_name, ".csv"),
  ) %>%
  separate(file_name, into = c("arm", "subject_id"), sep = "_") %>%
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  )
```

Let's see how it looks like now
```{r}
head(df_par, 5)
```

Next, we make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
df_par %>% 
  ggplot(aes(x = week, 
             y = observation, 
             group = subject_id, color = arm)) +
  geom_point() +
  geom_path() +
  facet_grid(. ~ arm) +
  labs(
    title = "Observations on each subject over time",
    x = "Week",
    y = "Observation"
  )
```

So we make the plot. Here are some difference between groups:
The experimental arm and control arm have similar average observations level at the beginning. As time goes by, the control arm keeps to be in the same level as the initial stage, but the experimental arm increases roughly linearly over time, and reaches a much higher level than the control arm.

## Problem 2

We first import the dataset downloaded.
```{r}
df_hom = 
  read_csv("data_p2/homicide-data.csv") 

head(df_hom, 5)
```

The raw data has `r nrow(df_hom)` rows and `r ncol(df_hom)` columns. The variables are: `r colnames(df_hom)`.   
Variable 'reported_date' is the reported date of homicide, and variables like 'victim_last' records the personal information of victim. Other variables like 'state' records the location of where the homicide took place.


From the dataset description, we know that the Washington Post collected data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities.

Next we create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

Note that there seems to be a error in an observation, where the city 'Tulsa' should correspond with the 'OK' state according to common sense and other columns, but was mistaken as 'AL'. So, I just fixed it.

```{r}
df_hom = 
  df_hom %>%
  mutate(
    state = ifelse(city == "Tulsa", "OK", state),
    city_state = str_c(city, ", ", state)
  ) %>%
  group_by(city_state) %>%
  summarize(
    n_tot_obs = n(),
    n_unsolved_obs = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

The 'df_hom' dataset now looks like:
```{r}
head(df_hom, 5)
```


Next, for the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
df_bal = 
  df_hom %>%
  filter(city_state == "Baltimore, MD")

prop.test(
  x = df_bal$n_unsolved_obs,
  n = df_bal$n_tot_obs
  ) %>%
  broom::tidy() %>%
  select(estimate, conf.low, conf.high)
```

Now we run prop.test for each of the cities in my dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. I will do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
df_unsolved = 
  df_hom %>%
  mutate(
    test = map2(
      .x = n_unsolved_obs,
      .y = n_tot_obs,
      ~ broom::tidy(prop.test(x = .x, n = .y))
    ) 
  ) %>%
  unnest(test) %>%
  janitor::clean_names() %>% 
  select(city_state, estimate, conf_low, conf_high)
```

The 'df_unsolved' dataset looks like:
```{r}
head(df_unsolved, 5)
```


Last, we will create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r fig.height = 10}
df_unsolved %>% 
  ggplot(
    aes(x = estimate, 
        y = fct_reorder(city_state, estimate)
        )
    ) +
  geom_point() +
  geom_errorbar(
    aes(xmax = conf_high, 
        xmin = conf_low)) +
  labs(
    title = "The estimates and CIs for each city",
    x = "Proportion Estimation",
    y = "Location: City, State"
  )
```

## Problem 3

First we set the following design elements:
```{r}
n = 30
sigma = 5
```

The simulation process can be written into a function:

```{r}
simulate = function(mu, n = 30, sigma = 5){
  x = rnorm(n = n, mean = mu, sd = sigma)
  t_test_result = t.test(x)
  t_test_result %>%
    broom::tidy()
}
```

Then we set $\mu=0$. Generate 5000 datasets from the model. For each dataset, save $\hat{\mu}=0$  and the p-value arising from a test of $H$ :$\mu$ = 0 using $\alpha$ =0.05.

```{r}
df_sim_mu0 = 
  expand.grid(
    mu = 0,
    iter = 1:5000
  ) %>%
  mutate(
    t_test_result = map(.x = mu,
                        ~ simulate(mu = .x)
                        )
  ) %>%
  unnest(t_test_result) %>%
  select(mu, estimate, p.value)
```

The first 5 rows of the result is:
```{r}
head(df_sim_mu0, 5)
```

Then we repeat the above process for $\mu$ = 1,2,3,4,5,6.

```{r}
df_sim_mu = 
  expand.grid(
    mu = c(1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) %>%
  mutate(
    t_test_result = map(.x = mu,
                        ~ simulate(mu = .x)
                        )
  ) %>%
  unnest(t_test_result) %>%
  select(mu, estimate, p.value)
```

And we can see the first 5 rows of the 'df_sim_mu' where $\mu$ could be 1,2,3,4,5,6.

Then we make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of $\mu$ on the x axis. Describe the association between effect size and power.

```{r}
df_sim_mu %>% 
  group_by(mu) %>% 
  summarize(
    power = sum(p.value < 0.05) / n()
  ) %>% 
  ggplot(aes(x = mu, y = power)) + 
  geom_point() +
  geom_path() +
  labs(
    x = "True means",
    y = "The power of test",
    title = "Plot: Power of Test")
```

The power of the test increases as the effect size increases and becomes far away from null hypothesis, but the increasing speed is gradually slower, and finally approaches near 1.

Then we make a plot showing the average estimate of $\mu$ on the y axis and the true value of $\mu$ on the x axis.
```{r}
df_sim_mu %>% 
  group_by(mu) %>% 
  summarize(
    avg_mu = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu, y = avg_mu)) + 
  geom_point() +
  geom_path() +
  labs(
    x = "True mean",
    y = "Average Mean Estimate",
    title = "Plot 1: Average Mean Estimate vs. True mean")
```

Then we make a second plot (or overlay on the first) the average estimate of $\mu$  only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis.
```{r}
df_sim_mu %>% 
  filter(p.value < 0.05) %>%
  group_by(mu) %>% 
  summarize(
    avg_mu = mean(estimate)
  ) %>% 
  ggplot(aes(x = mu, y = avg_mu)) + 
  geom_point() +
  geom_path() +
  labs(
    x = "True mean",
    y = "Average Mean Estimate (reject null)",
    title = "Plot 2: Average Mean Estimate (reject null) vs. True mean")
```

By comparing two plots, we can conclude that:  

At initial stage when the effect size is not large, the sample average of $\hat{\mu}$ is obviously unequal (larger) to the true value of $\mu$, given that the null hypothesis is rejected.  
When the effect size increases, the sample average of $\hat{\mu}$ becomes approximately equal to the true value of $\mu$. I think that's because with the increasing of effect size, we also have a larger power in the test, which leads to that phenomenon. This indicates that we may have a higher risk to reject the null when the power and effect size is small.

